---
toc: false
layout: post
title: Neural Circuit Policies (Part 5) - Sprase Neural Circuits
tags: [NCP, wormnet]
image: "images/thumb_sized/sparse.png"
description: Why quick overview of sparsity in machine learning and the NCPs

---

This is the fifth part in a series explaining our recent paper titled [Neural circuit policies enabling auditable autonomy](https://rdcu.be/b8sEo).

- [Part 1: Introduction]({{ site.baseurl }}/2020/09/14/wormnet1.html)
- [Part 2: An old neuron model revisted]({{ site.baseurl }}/2020/09/15/the_model.html)
- [Part 3: An unusual ODE-solver]({{ site.baseurl }}/2020/09/16/ode_solver.html)
- [Part 4: Training RNN is difficult]({{ site.baseurl }}/2021/02/18/rnn.html)
- **Part 5: Sparsity in machine learning**
- [Part 6: Pros and Cons of Neural Circuit Policies]({{ site.baseurl }}/2021/06/29/procon.html)


## Sparse neural networks

Neural networks consist of the multiplication of the inputs with weight matrices.

$$ y = Wx $$

The weight matrices W are typically fully populated, i.e., almost all entries are non-zero.
We call a weight matrix W and the corresponding neural network sparse if some fraction of W's entries become zero. In particular, a matrix where half of its entries are zero is said to have a 50% sparsity, and analogously, if 80% of all entries are zero, the matrix has a sparsity of 80%.

![t]({{ site.baseurl }}/images/wormnet/sparse/sparse.png "A neural network with three different levels of sparsity")


## Why do we want sparse models?
There are two potential reasons why we want our neural network to be sparse, computational efficiency and regularization.
When computing Wx and an entry of W is zero, we can save one multiplication and addition because the elements that are zero do not affect the output. Consequently, we can save lots of computations when W has a high sparsity level. There is an entire research area and industry that tries to exploit sparsity to increase the efficiency of neural networks. For example, the company Neural Magic focuses on sparsifying trained neural networks to make them run more efficiently on CPUs. 
The second reason why we might want sparsity, i.e., regularization, has to do with the fact that, when W is sparse, certain features are not used by the machine learning model. 
For example, the first output neuron of the neural network shown below

![t]({{ site.baseurl }}/images/wormnet/sparse/sparse2.png)

relies only on two input features. Moreover, the entire network does not use input feature number 3 at all.
This might be useful for certain datasets and machine learning models when the input includes features with no predictive power. A sparse model will omit such features, whereas a dense model might have a non-zero weight and include them for making a prediction.

### Sparsity in Biological Neural Networks
The C. elegans nervous system is highly sparse. In particular, it consists of 302 neurons, so an all-to-all wired network would have around 91k connections. However, C. elegans only have around 8k synapses, corresponding to a 91% sparsity level.
Speculatively, the sparsity of C. elegans' nervous system provides both the efficiency and regularization advantages discussed above, i.e., regularization in the sense that two unrelated neural circuits are not connected and thus no flawed dependency between the two circuits can be established.

## Pruning, Regularization, and Pre-defined Sparsity
Now that we know why we need sparse models, we can discuss **how** we can sparsify neural networks.
In particular, we can sparsity our model by:

- Adding an L1 weight regularization term to the training objective.
- Pruning the weight matrices during or after training.
- Enforcing pre-defined wiring on the weight matrices.

Options 1. and 2. both start with a fully connected weight matrix and continuously introduce sparsity to the network. In contrast, option 3. already starts with sparse wiring from the beginning of the training process. Therefore, if we want to investigate sparsity from a biologically plausible perspective, we have to with pre-defined sparse connectivity for our C. elegans-inspired neural networks.
 
## Training with Pre-defined Sparsity
To enforce fixed wiring during the training process, we have to make sure that matrix entries that are zero stay zero even after gradient-descent updates.
We implement this constraint by multiplying the trainable weight matrix W with a constant sparsity matrix S every time we use W in our machine learning model. The sparsity matrix consists of only 0 and 1 entries, i.e., 1 for present connections and 0 for two neurons without a connection. The weight matrix itself can be a standard fully populated matrix because the multiplication with S takes care of zeroing the entries.

$$ W \cdot S = $$
$$ \begin{pmatrix} w_{1,1} & w_{1,2} & w_{1,3} \\w_{2,1} & w_{2,2} & w_{2,3} \\ w_{3,1} & w_{3,2} & w_{3,3} \end{pmatrix} \cdot \begin{pmatrix} 0 & 1 & 0 \\1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} = \begin{pmatrix} 0  & w_{1,2} & 0 \\w_{2,1} & 0 & w_{2,3} \\ w_{3,1} & w_{3,2} & 0 \end{pmatrix} $$


The main reason why we choose this implementation is that we can easily implement it in deep learning frameworks like PyTorch and TensorFlow. For instance, a sparse layer can be realized in PyTorch as


{% highlight python %}
class SparseLinear:
    # expects 3 inputs and 3 outputs

    def __init__(self):
        self.W = torch.nn.Parameter(data=torch.empty((3,3)), requires_grad=True)
        self.S = torch.tensor([[0, 1, 0], [1, 0, 1], [1, 1, 0]])

    def reset_parameters(self):
        init.kaiming_uniform_(self.W, a=math.sqrt(5))
        
    def forward(self, input):
        return torch.mm(input, self.W*self.S)

{% endhighlight %}

An alternative would have been to parametrize W directly as a sparse Tensor object, e.g., both TensorFlow and PyTorch have sparse Tensor objects.
However, this way, the deep learning framework would have to rely on sparse matrix-vector multiplication routines every time we use W, which can be slower than the routines for densely populated matrices.

## Conclusion
In essence, here, we explained the different use-cases of sparsity in machine learning and introduced a way to enforce pre-defined wiring on our neural network architectures.
In the [next and last part](({{ site.baseurl }}/2021/06/29/procon.html)) of the series, we will discuss the benefits and costs involved with our C. elegans-inspired machine learning models.
 

