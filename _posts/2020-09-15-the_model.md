---
toc: false
layout: post
title: Part 1 - An old neuron model revisted
tags: [NCP, wormnet]
image: "images/wormnet/circuit/the_model.png"

---

This is the second part in a series explaining our recent paper titled XXX.

## Neurons are cells

As briefly discussed in the first part, the neuron model used in machine learning is quite a high-level abstraction of the complex cells neuroscience deals with. 
We have also talked about how the activation value of a neuron is actually an electrical potential caused by different concentrations of charged ions inside and outside the cell. 
Going one step further, there may be different ion concentrations at different locations inside the cell, resulting in the neuron not having a single activation value but a location-dependent potential.
We won't go this far of construction such a compartmental model, but assume a neuron has a single electrical potential.
We start by modeling the neuron's membrane that separates the ions inside and outside the cell as a capacitor. 

$$ C \frac{d v}{d t} = i(t) $$

The voltage the capacitor is charged to represent the activation of the neuron. The idea of modeling a neuron's membrane as a capacitor dates back to 1907 [^1] and is used is many influencial neuron models [^2].
Next, we assume the neuron has a resting potential: the potential it wants to attain when no external stimulus is applied to the neuron.

![award]({{ site.baseurl }}/images/wormnet/circuit/rc.png "A neuron is modelled as a capacitor")

A network of such standalone RC-circuits is quite boring, so we need a synapse model to let the neurons communicate with each other.
Physical synapses trigger an increase of certain ions in the post-synaptic neuron through neurotransmitters released by the pre-synaptic neuron. As a result, the post-synaptic neuron's potential increases or decreases, depending on the ions' flow direction and polarity. The release of neurotransmitters itself is triggered by pre-synaptic neuron's potential exceeding a certain threshold.
We model this process by adding the following electrical circuit to the post-synaptic neuron,

![award]({{ site.baseurl }}/images/wormnet/circuit/syn.png "A neuron connected by two incomming synapses 1 and 2")

where $$E_{rev,}$$ determines the reverse-potential, i.e., the potential toward which the synapses pushes the post-synaptic neuron's potential. In the case of an excitatory synapse, this value is larger than the resting potential (activating the neuron). In the case of an inhibitory synapse, $$E_{rev,k}$$  is lower than the resting potential (deactivating the neuron).

The interesting **non-linear** dynamics of the synapse happens inside the varistor $$g_{k}$$ . The conductance of it is governed by the equation 

$$ g_k = \frac{G_k}{1+\exp{(-\sigma  v_{pre,k} + \mu_{k})}} $$

, where $$v_{pre,k}$$ is the pre-synaptic neuron's potential, $$G_k,\sigma_k,\mu_k $$ weights of the synapse.

Now that we have a neuron model and a synapse model to let the neurons talk to each other, we still miss a critical part of a neural network: the inputs and outputs. In C. elegans' nervous system, physical inputs are introduced into the network through sensory neurons, i.e., neurons whose potential is influenced by external stimulus. Analogously, dedicated motor neurons take care of producing a physical response from the information processing inside C. elegans' nervous system. 

In our computational model, we will treat motor neurons as normal neurons and define the network's output as the potential of these motor neurons.
Moreover, we define the input variables' values as the neuron potential of a separate set of sensory neurons. Thus, the potential of these sensory neurons cannot be affected by synapses, as it is determined solely by the values of the inputs. Consequently, sensory neurons can only have outgoing synapses but no incoming links.

As the input and output variable might be differently scaled than the neuron values inside the network, we re-scale the input and output signals by element-wise affine transformations.

Finally, we can write down the full model. Let's say X are the inputs of the network and Y are the outputs of the network. 


## Is this really used in neuroscience?
The model introduce above does not originate from our imagination, but is actually taken from the book *Methods in Neuronal Modeling - From Ions to Networks* [^3] by Christof Koch and Idan Segev and has been used in neuroscience research[^4]]. 
There is one difference though between our model and the one defined in the neuroscience book. The model described by Koch and Segev models each synapse also via a differential equation, i.e., giving every synapse a possibly different temporal behavior. 
Our modification replaces the synapse ODEs by their steady-state solution for $$t \rightarrow \infty $$. This simplification significantly reduces the memory requirement to store the RNN-state from squared to linear, while losing a bit of expressive power.
We called the resulting model the Liquid time constant (LTC) model. Why we named it that way and mathematical properties about it can be found in our arXiv preprint[^5]. 

## References

[^1]: Abbott 1999. [Lapicqueâ€™s introduction of the integrate-and-fire model neuron (1907)](https://www.sciencedirect.com/science/article/abs/pii/S0361923099001616)
[^2]: [Hodgkin-Huxley Model](https://neuronaldynamics.epfl.ch/online/Ch2.S2.html)
[^3]: Koch and Segev [*Methods in Neuronal Modeling - From Ions to Networks*](https://mitpress.mit.edu/books/methods-neuronal-modeling-second-edition)
[^4]: Wicks et al 1996. [A Dynamic Network Simulation of the Nematode Tap Withdrawal Circuit: Predictions Concerning Synaptic Function Using Behavioral Criteria](https://www.jneurosci.org/content/jneuro/16/12/4017.full.pdf)
[^5]: [Liquid Time-constant Networks](https://arxiv.org/abs/2006.04439)