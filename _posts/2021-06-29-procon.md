---
toc: false
layout: post
title: Neural Circuit Policies (Part 6) - Pro and Cons of Brain-inspired ML Models
tags: [NCP, wormnet]
image: "images/thumb_sized/procon.png"

---

Developing a brain-inspired machine learning model is no easy task.
Particularly, this raises the question of whether all the effort one has to put into [Neural circuit policies (NCPs)](https://rdcu.be/b8sEo) is really worth it.
Here, first, we will recap these costs and efforts of brain-inspired ML and then talk about the benefits they provide.

## Cons: Why not to use NCPs
### ODE-based ML models are slow
Machine learning models that are based on ordinary differential equations (ODE), such as our NCPs, are slower regarding both training and inference time.
A standard feedforward or recurrent neural network can simply compute its output by a single function evaluation. 
Conversely, an ODE-based model requires an **ODE-solver** that simulates the dynamics of the system in order to compute an output. Specifically, the ODE-solver has the evaluate the dynamics function defining the model at **several intermediate points** when computing the output. This property makes them much slower than the standard models.
For example, the ODE-solver we use in our NCP implementation uses by default 6 intermediate evaluation points. Simply put, the NCP's ODE-solver makes it 6 times slower compared to a standard RNN of the same size.

### Small models are not necessarily faster
One may argue that the small size of NCP models makes them faster than standard models. However, this statement is not necessarily true. In practice, we train and run most machine learning models on GPUs, which are based on an inherently parallel computing model. For instance, an [NVIDIA 1080TI GPU](https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877) from 2017 has 3584 compute cores running concurrently.
Now, imagine we want to evaluate a 19-neuron NCP network on a single input sample. Although we can compute all 19 neurons in parallel, we only use 19 out of 3584 available compute cores (~0.5%). 
Moreover, imagine we upgrade our system to a more recent [NVIDIA RTX 3090](https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622), which has 10496 compute units, then we won't observe any speedup running our 19-neuron NCP model.
The main issue described above is that most small ML models' bottleneck is their **sequential computation** allows little to no parallelization on GPU and other parallel hardware accelerators.
One of the reasons why we saw the trend of bigger and bigger ML models coming out of research labs is that they scale well when better and bigger hardware becomes available.

### Sparse models are not necessarily faster
As outlined in a [separate blog article](https://mlech26l.github.io/pages/jupyter/sparsity/2022/07/04/sparsemath.html), making a model sparse does not necessarily make it faster.
In a nutshell, this is because arithmetic operations on dense (fully populated) matrices have very **predictable uniform compute and memory access characteristics**. Consequently, one needs a very high sparsity level to see an actual improvement in efficiency compared to a fully connected model.
For instance, an NCP with 80% sparsity may not be faster than an NCP model with no sparsity due to the effect described above.

### NCPs suffers from a vanishing gradient
Because the underlying neuron model suffers from a vanishing gradient, NCPs **cannot learn long-term dependencies** in the data.
Many sequential datasets express some form of relationship between two temporally separated samples. Thus, the NCPs are suited only for specific types of sequential data, i.e., they have only short-term temporal dependencies.

### RNNs are dead
The Transformer has de-facto replaced RNNs as the standard machine learning model in sequential data processing such as natural language processing (NLP). Although Transformers may not be the most efficient network architecture, they allow straightforward **parallelization** (see point above) and are more **stable** to train, i.e., they do not suffer from the vanishing/exploding gradient on the same level as RNNs do.
Consequently, one may argue that RNNs, including the NCP, have no future when facing Transformers and models derived from the same principle.

## Pros: Why use NCPs
### NCPs can be efficient
Although NCPs and sparse models cannot run efficiently on GPU devices, this does not imply that NCPs themselves are inefficient. In particular, on sequential computing devices such as **embedded CPUs**, the small size and sparsity of NCPs can make them faster than standard neural network counterparts. 

### Learning better causal models
Causality in machine learning is a complex topic. Turing award winner [Judea Pearl](https://www.kdnuggets.com/2018/06/gray-pearl-book-of-why.html) showed the impossibility of inferring the correct causal dependency between variables from passive observations alone. To infer an accurate causal structure from data, we either need to perform active interventions or guide our model with a prior. As doing active interventions is not always feasible, e.g., training an autonomous vehicle by letting it perform active interventions will result in crashes, we prefer the second option of equipping our model with a **causal prior**. 
For certain types of tasks and data, our NCPs may provide such a prior.
As outlined by [Bernhard Sch√∂lkopf](https://arxiv.org/pdf/1911.10500.pdf), ODE-based models are a good choice for adequately modeling the effects in physical systems. Moreover, the brain-inspired sparsity of the NCP wiring may also help to separate unrelated signals.

### NCPs are an idea, not an ML model
The main purpose of NCPs is as a **research platform**; to better understand how intelligent decision-making emerges in various types of information processing paradigms. 
NCPs are not the destination but a practical step on the journey toward this goal. NCPs allow us to study the flaws of existing machine learning models and characterize the causal implications of datasets. Specifically, we have demonstrated that NCPs are well suited for end-to-end vehicle and drone navigation scenarios.
The next step toward our goal is to address the limitations of NCPs described above and scale them to more complex tasks.


## Conclusions: NCPs have their niche
All the advantages and disadvantages described above are valuable observations made in our research. We have seen that brain-inspired can outperform existing machine learning models in real-world tasks.
However, we have also seen that they are no silver bullet but come attached with a long list of drawbacks.
Nonetheless, NCPs have their **niche set of applications** where they serve as a useful machine learning model.
