---
toc: false
layout: post
title: The dangers of biases in AI
description: An examples of how bias manifests in AI systems
tags: [Bias, AI]
image: "images/biases/thumb.jpg"
---

## Background

At the [2020 Conference on Computer Vision and Pattern Recognition (CVPR)](http://cvpr2020.thecvf.com/) Duke University published a new method title [Photo Upsampling via Latent Space Exploration (PULSE)](https://arxiv.org/pdf/2003.03808.pdf) for upsampling low resolution photos.
In a nutshell, PULSE searches the latent space of a generative model to find a high-resolution image that downsamples to an image that looks similar to the low-resolution input image. 
In particular, the authors achieved fascinating looking results by using  [StyleGAN](https://github.com/NVlabs/stylegan) trained on the [FFHQ](https://github.com/NVlabs/ffhq-dataset) dataset of high-resolution images of human faces.

Indeed, if we run PULSE with downsampled images generated by StyleGAN itself, we get remarkable detailed results:

![synthetic]({{ site.baseurl }}/images/biases/synthetic.png "Synthetic images (generated by PULSE/StyleGAN)")

Nonetheless, the method has sparked extensive discussions about biases in AI. 
It all started with a tweet showing how PULSE upscaled a low-res picture of Barack Obama to a tanned white person.

{% twitter https://twitter.com/Chicken3gg/status/1274314622447820801?s=20 %}

The authors explicitly state that *the objective of PULSE is not to reconstruct the original image, as this is impossible due to the lack of information in the low-res image*. 
Nonetheless, one would never expect the low-res image of Barack Obama to be upscaled to the picture shown above.

The authors claim that biased training data of StyleGAN most likely causes the observed bias. The used FFHQ dataset primarily comprises of faces of white people. 
They hypothesize that, due to this training data imbalance, the explored parts of the latent space by PULSE correspond predominately to white people's faces.

## The dangers of biased AI

Despite this claimed lack of training data diversity, what's fascinating and dangerous at the same time is how this bias manifests in the AI.
To examine this, let's downsample the Dancing Pallbearers and see what images PULSE will generate.

![synthetic]({{ site.baseurl }}/images/biases/out.png "Upscaled Dancing Pallbearers (generated by PULSE/StyleGAN)")

While the synthetic low-res images share color and shape profile of the corresponding real picture, we notice a crucial property in each upsample image: It's always a white person. 
Surprisingly, the AI comes up with subtle reasons for explaining the skin tone and head covering in the input image. 
The skin tone is not caused by dark skin color but because the person in the synthetic image stands in a shaded area.
The white stripes on the hats are explained by sun rays hitting on that part of the person's face.
Moreover, the black area from the top hat in the first image is explained by the high volume of the person's hair.

We see that the AI finds *creative* ways to explain the features observed in the test images that weren't there during training. This is precisely where the danger of biased AI systems lie. 
The AI can find solutions that are valid on a mathematical level (L2-distance) but unacceptable on a [societal level](https://iclr.cc/virtual_2020/speaker_3.html).
We can repeat the experiments, but each time PULSE comes up with the same explanations.

![synthetic]({{ site.baseurl }}/images/biases/all_seeds.png "Upscaled Dancing Pallbearers different random seeds (generated by PULSE/StyleGAN)")

## Bias beyond mathematics

The authors of the original paper performed an additional experiment to evaluate the **''success rate''** of PULSE of various groups (Female/Male, Black, East Asian, Indian, etc. ).
Their definition of success only concerns if the method finds an image that downsamples to a similar-looking image as the low-resolution input with respect to a pixel-based distance metric or not.
The authors found a marginal but non-significant difference in ''success rate'' among the tested groups. 
Essentially, as I stated above, on a mathematical level, the authors were unable to characterize a significant bias in the system.

## Conclusion

Bias in machine learning is a sensitive topic that cannot be reduced to mathematics and training data imbalance alone. It mandates examinations involving societal and domain-specific experts.





