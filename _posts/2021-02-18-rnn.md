---
toc: false
layout: post
title: Neural Circuit Policies (Part 4) - Training RNNs is Difficult
tags: [NCP, wormnet]
image: "images/thumb_sized/rnn.png"
description: How the vanishing and exploding gradients make training RNNs difficult

---


This is the fourth part in a series explaining our recent paper titled [Neural circuit policies enabling auditable autonomy](https://rdcu.be/b8sEo).

- [Part 1: Introduction]({{ site.baseurl }}/2020/09/14/wormnet1.html)
- [Part 2: A Hundred-Year-Old Neuron Model]({{ site.baseurl }}/2020/09/15/the_model.html)
- [Part 3: An Unusual ODE-solver]({{ site.baseurl }}/2020/09/16/ode_solver.html)
- **Part 4: Training RNNs is Difficult**
- [Part 5: Sparsity in Machine Learning]({{ site.baseurl }}/2021/03/08/sparsity.html)
- [Part 6: Pros and Cons of Neural Circuit Policies]({{ site.baseurl }}/2021/06/29/procon.html)


## Recurrent neural networks

Most real-world tasks involve some form of sequential aspects. For instance, when we read a book and want to know why Alice jumps off an airplane on page 384, we first have to read pages 1 to 383 to understand who Alice is and what led her to this action.

For many practical machine learning applications, we ignore the temporal aspects of the data. Image classification is a perfect example of where we learn from static images compared to the continuous visual streams we humans observe.
However, for other applications, like our book example from above, temporal information is vital for our ML model to capture meaningful patterns. 
For instance, if Bob has a fever, should we worry about him? Only by knowing the past information of how his fever was an hour ago can we infer if Bob is recovering or if his sickness is getting worse. 

Building machine learning models that process such sequential data requires us to think about how to feed historical data into our neural network. 
One option is to simply run the entire history or a subset through a standard feedforward network. 

![]({{ site.baseurl }}/images/rnn/ff.webp "A feedforward network processing sequential data")

However, storing and feeding the entire history into a network for every single prediction is inefficient. 
Moreover, keeping only a subset of the history is also not a general approach, as we don't know how long our temporal lookback window should be. Having a short window may discard important information, and having a long window comes with an increase in computational cost. 
For example, the Transformer architecture uses a such window-based approach to process sequential natural language inputs. Typically, the context length of these Transformers is capped to 512 or 1024 tokens due to the computational cost, e.g., the well-known [BERT model](https://arxiv.org/pdf/1810.04805.pdf) was trained with a cap of 512 tokens.

Recurrent neural networks (RNNs) are machine learning models for efficiently processing sequential data.
Instead of explicitly storing the entire input history, RNNs learn to condense important information into a compact hidden state. 
At each processing step, the RNN observes an input and its current hidden state, then computes the output and updates its hidden state.

![]({{ site.baseurl }}/images/rnn/rnns/feedback.png "A recurrent neural network (RNN)")

Consequently, we don't need to keep historical input data, which makes RNNs memory efficient. On top of that, for making an additional prediction we only need to feed two variables into the network, i.e., the input and the hidden state, which makes RNNs computationally efficient.

![]({{ site.baseurl }}/images/rnn/rnn_processing.webp "A RNN processing sequential data")


## So is it all just sunshine and roses?
Not really. 
While RNNs have intriguing computational advantages, they have one fundamental challenge: Training them is hard. 
When we examine the unrolled representation of an RNN, we notice that the computation graph is usually extremely deep, e.g., 1000 layers in the example below. However, different from a very deep feedforward network, all layers are parametrized by the same weights. Moreover, there is potentially a separate objective at each step, i.e., during training, the aggregated loss of all output prediction errors is minimized.
These properties already provide an initial sense of why training such a network can be challenging. For instance, the frequent re-use of the weights may create conflicts in the gradient update resulting in a flat optimization surface.
One particularly devastating effect is the vanishing and exploding gradient phenomenon. 

In his [1991 diploma thesis](https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf), Sepp Hochreiter first discovered that the error flow of the backpropagation algorithm in RNNs realizes a power series. 
The series either exponentially grows or decays with the number of RNN steps in the training sequences. An exponentially growing error flow, i.e., an exploding gradient, makes the training process unstable. In the best cases, the training does not converge as the weights start to oscillate. In the worst case, an exploding gradient causes numerical overflows, i.e., NaNs, preventing any form of training. 

A vanishing gradient has less severe impacts on the training outcome, as RNNs with a vanishing gradient usually converge without numerical errors. However, they are unable to capture long-term dependencies in the training data. 
Imagine our training data express the following dependency: The input event "mouse" causes and output event "cat" downstream somewhen in the future. 
Let's examine what is necessary for our RNN to learn such causal relations:

![]({{ site.baseurl }}/images/rnn/rnns/dependency.png "A example causal dependency between an input event (mouse) and an corresponding output event (cat) that a neural network should learn.")

The vanishing gradient makes the cat's error signal decrease when propagated backward during learning. However, as the time span between the two events is short, the signal has not entirely vanished when arriving at the mouse event.
Consequently, the RNN can still capture such short-term dependency.

![]({{ site.baseurl }}/images/rnn/short_term.webp "A RNN suffering from a vanishing gradient is still able to learn short-term dependencies")

Now, let's say there is a long time span between the two events. 

![]({{ site.baseurl }}/images/rnn/long_term.webp "A RNN suffering from a vanishing gradient cannot learn long-term dependencies")

In this case, the power series realized by the backpropagation makes the error signal decay toward zero. 
The error signal arriving a the "mouse" event has become too weak to make meaningful updates to the RNN parameters. Consequently, the RNN cannot learn to remember the "mouse" event and learn the causal relationship between the two temporally separated events.

## Can we fix a vanishing gradient?
We can avoid the vanishing gradient problem by ensuring that the backpropagation algorithm's power series preserves the error's magnitude when transporting it backward in time.
Mathematically, this is the case if the singular values of the state-to-previous state Jacobian matrix of the RNN are equal to 1.
RNNs, where the update step is a multiplication with a unitary matrix, fulfill precisely this property. 
However, ensuring the updated matrix is unitary while being fully trainable is a non-trivial task, e.g., see [(Arjovsky, Shah, and Bengio, 2016)](https://arxiv.org/pdf/1511.06464v4.pdf) for more detail.
Alternatively, we can fix the update step as an identity/unitary matrix and only train gates that access (read/write) the RNN memory. This is how the infamous [long-short-term memory (LSTM)](http://www.bioinf.jku.at/publications/older/2604.pdf) tackles the vanishing and exploding gradient problem.

Enforcing an identity/unitary RNN update comes with disadvantages as well. 
In particular, an identity/unitary matrix operates the RNN as a memory cell that tries to remember everything. This can be problematic for some datasets, where we don't want to remember events for a long period. For example, the recommendation of a lane-keeping assistance system should depend only on the observations of the past few seconds. For unitary RNNs and the LSTM, this means that the network needs to learn to forget and clear its memory over time. An RNN that suffers from a vanishing gradient, however, already implicitly forgets past events and might perform better on such a machine learning task.
Adding a forgetting mechanism to the RNN memory, e.g., the forget gate in an LSTM, mitigates this issue but introduces a potentially vanishing factor to the RNN state update. 

## Do feedforward networks also suffer from a vanishing gradient?
Feedforward neural networks also suffer from a vanishing gradient but to a much lesser extent than RNNs. 
First of all, the backpropagation chain of feedforward networks is much shorter than for RNNs. Let's consider the BERT examples from above, i.e., processing a sequence of 512 tokens. The feedforward BERT-base network used in the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) consists of 24 layers (12 Transformer encoder blocks), thus our backpropagation chain is of length 24.
However, if we used an RNN instead of the feedforward Transformer model, we would need to backpropagate through the entire sequence of 512 layers.

Another reason why the vanishing and exploding gradient is less of a concern for feedforward neural networks is that there exist very effective methods for addressing the problem in feedforward settings.
In particular, normalization layers (e.g., [batch-normalization](https://arxiv.org/abs/1502.03167) and [layer-normalization](https://arxiv.org/abs/1706.03762)), [adaptive learning rates](https://arxiv.org/pdf/1412.6980.pdf), and [residual connections](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) [^1].
Normalization and adaptive learning rates are less effective in RNNs because every step in the backpropagation chain uses the same layer, whereas the layers in a feedforward network are independent.


[^1]: Residual connections can be thought of as analogous to having a fixed identity/unitary pathway in RNNs [(Srivastava, Greff and Schmiduber, 2015)](https://arxiv.org/pdf/1505.00387.pdf).
 

## What does this mean for the Neural Circuit Policies?
In our [paper](https://rdcu.be/b8sEo), we have outlined that the neural model of the NCPs suffers from a vanishing gradient. Hence, the training process of NCPs might be slow to converge, and the model cannot capture long-term dependencies in the data. This makes them an exciting choice for datasets where we don't want to remember events for a long time. However, many sequential datasets contain important long-term temporal relations, which means, unfortunately, the NCPs are excluded from providing any advantage on these machine learning tasks.

In the [next part (Sparsity in Machine Learning)]({{ site.baseurl }}/2021/03/08/sparsity.html), we will talk about how we can impose a strucutre on the wiring of a neural network.





 

