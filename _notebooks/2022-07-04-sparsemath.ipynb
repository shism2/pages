{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Neural Networks are Slow (Sometimes)\n",
    "> An example of how the sparsity level affects runtime of sparse matrix operations\n",
    "\n",
    "- toc: false \n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [jupyter, sparsity]\n",
    "- image: images/thumb_sized/sparsemath.png\n",
    "- author: Mathias Lechner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprase neural networks\n",
    "\n",
    "Pruning is a well-established technique for compressing neural networks before deployment in order to make them run faster.\n",
    "The compression process removes connections from the network that are considered unimportant. The pruned connections have only a minor impact on the total accuracy of the network but potentially speedups the inference of the network as fewer connections need to be computed.\n",
    "\n",
    "![](https://mlech26l.github.io/pages/images/wormnet/sparse/sparse.png)\n",
    "\n",
    "On a more technical level, the connections inside a neural network layer are realized as a matrix multiplication:\n",
    "\n",
    "$$ y = ReLU\\Big(\\ \\begin{pmatrix} 0.05  & -1.2 & -0.12 \\\\2.7 & 0.09 & 0.73 \\\\ -0.62  & 0.3 & -0.04 \\end{pmatrix}\\ \\cdot x\\Big) $$\n",
    "\n",
    "The pruning algorithm removes connections from the network by setting the corresponding entries in the weight matrix to zero, i.e., making the matrix sparse. The semantics after pruning of the layer may look something like\n",
    "\n",
    "$$ y = ReLU\\Big(\\ \\begin{pmatrix} 0  & -1.2 & 0 \\\\2.7 & 0 & 0.73 \\\\ -0.62  & 0.3 & 0 \\end{pmatrix}\\ \\cdot x\\Big) $$\n",
    "\n",
    "Despite the pruned matrix looking much simpler than the densely populated one, turning this sparsity into an improvement in computational efficiency is not trivial.\n",
    "This is because a dense matrix can be stored in an array, i.e., a consecutive block of memory where each item takes up the same number of bytes. In this array format, we don't worry about which matrix entry belongs to which array index, as they follow the relationship\n",
    "$$ W_{i,j} = \\text{array}[i \\cdot \\text{num_columns} + j] $$\n",
    "where *num_columns* corresponds to the number of rows of the matrix.\n",
    "\n",
    "For sparse matrices, however, using this array format would not provide any memory or computation benefits because there would be a large amount of redundant zero stored in the array.\n",
    "The [compressed sparse row (CSR)](https://en.wikipedia.org/wiki/Sparse_matrix) and other sparse formats avoid this problem by storing only the non-zero entries of the matrix. Contrary to the dense array format, the CSR scheme needs to store not only the values of the matrix but also the indices, i.e., to distinguish between zero and non-zero entries. For instance, a matrix in the CSR format is represented in the form of three arrays, i.e., the column indices, the row indices, and the values themselves.\n",
    "Let's visualize this on a code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A= [[-0.          0.         -0.        ]\n",
      " [ 0.         -0.          0.        ]\n",
      " [ 0.          0.          0.24214645]]\n",
      "A_sparse=   (2, 2)\t0.24214644868677146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def create_sparse_mat(size, sparsity_level):\n",
    "    rng = np.random.default_rng(int(size*100*sparsity_level))\n",
    "    mask = rng.choice(2,size=(size,size),p=[sparsity_level,1.0-sparsity_level])\n",
    "    mat = rng.normal(size=(size,size))\n",
    "    mat = mat * mask\n",
    "    sparse_mat = csr_matrix(mat)\n",
    "    return mat,sparse_mat\n",
    "\n",
    "A,A_sparse = create_sparse_mat(3,0.8)    \n",
    "print(\"A=\",A)\n",
    "print(\"A_sparse=\",A_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense and Sparse matrix operations\n",
    "\n",
    "Obviously, the algorithms for implementing matrix operations, e.g., matrix-vector or matrix-matrix products, look quite different for densely and sparsely represented matrices. Without going into too much detail, on the one hand, the dense matrix-vector algorithm needs to iterate over a single block of memory, i.e., the array storing the matrix. On the other hand, a sparse matrix-vector algorithm also needs to traverse over three arrays, i.e., the columns, the rows, and the values, tripling the amount of memory that needs to be accessed [^1].\n",
    "In a nutshell, a sparse representation adds some overhead to matrix operations compared to densely stored matrices.\n",
    "\n",
    "The question that we now have is, at what sparsity level do the sparse matrix operations become more efficient than their dense counterparts?\n",
    "We can make a first rough estimation from the memory requirement of the CSR format. As the amount of memory is at least double the number of non-zero entries, we can expect sparse operations to be slower if the sparsity level is below 50%.\n",
    "We try to answer this question empirically to get a more accurate picture. In particular, we look at the runtime of matrix-vector products and vary the number of matrix entries that are zeros:\n",
    "\n",
    "[^1]: The CSR format compresses the space required to store the row indices. Thus, the required memory is less than triple the amount for a dense matrix, but nonetheless, more than double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9c020b613b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Warump CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msparsity_levels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sparse_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdense_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure_dot_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c136011c681f>\u001b[0m in \u001b[0;36mcreate_sparse_mat\u001b[0;34m(size, sparsity_level)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_sparse_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msparsity_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msparsity_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msparsity_level\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def measure_dot_time(mat,x,n=20):\n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(n):\n",
    "        x = mat.dot(x)\n",
    "    return 1000.0*(time.perf_counter()-start_time ) # in milliseconds\n",
    "\n",
    "SIZE = 8192\n",
    "sparsity_levels = [0,0.3,0.5,0.8,0.85,0.9,0.95,0.97,0.99]\n",
    "dense_times, sparse_times = [],[]\n",
    "[np.eye(SIZE).dot(np.ones(SIZE)) for i in range(10)] # Warump CPU\n",
    "for sp in sparsity_levels:\n",
    "    A, A_sparse = create_sparse_mat(SIZE,sp)\n",
    "    x = np.random.default_rng(123).normal(size=SIZE)\n",
    "    dense_times.append(measure_dot_time(A,x))\n",
    "    sparse_times.append(measure_dot_time(A_sparse,x))\n",
    "    \n",
    "df = pd.DataFrame.from_dict({\"Sparsity\":sparsity_levels, \"Dense times\": dense_times, \"Sparse times\": sparse_times })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two extreme cases, i.e., 0% and 99%, we can see a significant difference in terms of performance. For a fully populated matrix, the matrix-vector product is three times slower than the operation carried out in a sparse storage format. Conversely, if the matrix consists of 99% zero entries, the sparse representation is approximately 20 times faster.\n",
    "  \n",
    "Let's plot our results to get a nice visual presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(df[\"Sparsity\"],df[\"Dense times\"])\n",
    "ax.plot(df[\"Sparsity\"],df[\"Sparse times\"])\n",
    "ax.set_xlabel(\"Sparsity level\")\n",
    "ax.set_ylabel(\"Runtime (milliseconds)\")\n",
    "ax.legend([\"Dense matrix\",\"Sparse matrix\"],loc=\"upper right\")\n",
    "ax.set_xticks(np.linspace(0,1,11))\n",
    "ax.set_title(\"Matrix-vector product runtime\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that below 80% sparsity, the dense matrix operations clearly outperforms the CSR matrix-vector product.\n",
    "It takes a sparsity level of around 80% for the operation to catch up and perform on par with the dense format. \n",
    "A meaningful speedup is measured starting from a 90% sparsity level. \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have seen that a sparse neural network is not necessarily more efficient than a fully-connected model. \n",
    "Our experiments have shown a sparsity level of above 90% is necessary to see any advantage when running our networks on the CPU.\n",
    "Note that we might need a much higher sparsity level on highly parallel GPU devices before seeing any performance improvement compared to dense matrix operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
