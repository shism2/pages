{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Myths about quantized neural networks\n",
    "> A post about misconceptions and myths around quantization of neural networks\n",
    "\n",
    "- toc: false \n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [jupyter, quantization]\n",
    "- image: images/die.jpg\n",
    "- author: Mathias Lechner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: What are quantized neural networks? \n",
    "\n",
    "When we run numerical algorithms on our computer, we need to make sacrifices in terms of precision for the sake of runtime. \n",
    "For instance, the square root of 2 is an irrational number and has an infinite amount of decimal digits. \n",
    "Thus we need to decide how many digits we really need for our application.\n",
    "Each extra digit of precision increases the memory and time requirements to store and compute a variable.\n",
    "\n",
    "For example, the IEEE-754 standard specifies four types of floating-point formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.sqrt(2):\n",
      "float128: 1.4142135623730950488\n",
      "float64:  1.4142135623730951\n",
      "float32:  1.4142135\n",
      "float16:  1.414\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "import numpy as np\n",
    "print(\"np.sqrt(2):\")\n",
    "print(\"float128:\",str(np.sqrt(np.float128(2))))\n",
    "print(\"float64: \",str(np.sqrt(np.float64(2))))\n",
    "print(\"float32: \",str(np.sqrt(np.float32(2))))\n",
    "print(\"float16: \",str(np.sqrt(np.float16(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning applications, the ```float32``` format has been the default choice, as it provides a decent performance while avoiding extreme numerical errors. \n",
    "However, in the past decade researcher have made the following two observations:\n",
    "\n",
    "- During the training phase, certain types of layers can be run and trained with lower precision (e.g., ```float16```)\n",
    "- After the training phase (=inference phase), neural networks can run with much lower precision levels without sacrificing much accuracy\n",
    "\n",
    "Consequently, Nvidia's latest [A100 GPU](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf) supports the following six numerical format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data type</th>\n",
       "      <th>Significand precision</th>\n",
       "      <th>Exponent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>float64</td>\n",
       "      <td>52-bits</td>\n",
       "      <td>11-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>float32</td>\n",
       "      <td>23-bits</td>\n",
       "      <td>8-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TensorFloat32</td>\n",
       "      <td>10-bits</td>\n",
       "      <td>8-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>float16</td>\n",
       "      <td>10-bits</td>\n",
       "      <td>5-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfloat16</td>\n",
       "      <td>7-bits</td>\n",
       "      <td>8-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>int8</td>\n",
       "      <td>7-bits</td>\n",
       "      <td>0-bits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Data type Significand precision Exponent\n",
       "0        float64               52-bits  11-bits\n",
       "1        float32               23-bits   8-bits\n",
       "2  TensorFloat32               10-bits   8-bits\n",
       "3        float16               10-bits   5-bits\n",
       "4       bfloat16                7-bits   8-bits\n",
       "5           int8                7-bits   0-bits"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "df = pd.DataFrame({'Data type': [\"float64\"],\n",
    "        'TOPS': [9.7],\n",
    "        'Significand precision': ['52-bits'],\n",
    "        'Exponent': ['11-bits'],\n",
    "        'Comment': ['Double precision IEE-754 floating-point'],\n",
    "        })\n",
    "df = df.append({'TOPS': 19.5,\n",
    "        'Data type': \"float32\",\n",
    "        'Comment': 'Single precision IEE-754 floating-point',\n",
    "        'Significand precision': '23-bits',\n",
    "        'Exponent': '8-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 156,\n",
    "        'Data type': \"TensorFloat32\",\n",
    "        'Comment': '32-bit floating-point format with reduced significand precision',\n",
    "        'Significand precision': '10-bits',\n",
    "        'Exponent': '8-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 312,\n",
    "        'Data type': \"float16\",\n",
    "        'Comment': 'Half precision IEE-754 floating-point',\n",
    "        'Significand precision': '10-bits',\n",
    "        'Exponent': '5-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 312,\n",
    "        'Data type': \"bfloat16\",\n",
    "        'Comment': '16-bit brain-float format with larger range but reduced significand precision',\n",
    "        'Significand precision': '7-bits',\n",
    "        'Exponent': '8-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 624,\n",
    "        'Data type': \"int8\",\n",
    "        'Comment': '8-bit integer format for fixed-point arithmetic',\n",
    "        'Significand precision': '7-bits',\n",
    "        'Exponent': '0-bits',\n",
    "        },ignore_index=True)\n",
    "df[['Data type','Significand precision','Exponent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One item that stands out in this list is the last row: While all other formats are based on a floating-point representation, ```int8``` is an integer type.\n",
    "\n",
    "This raises the question: \n",
    "\n",
    "> **How can we run a neural network with integer operations?**\n",
    "\n",
    "The answer is quantization. \n",
    "\n",
    "> Quantization translates a network that operates over floating-point variables into a network that uses fixed-point arithmetic\n",
    "\n",
    "\n",
    "[Fixed-point arithmetic](https://en.wikipedia.org/wiki/Fixed-point_arithmetic) is a numerical format that can be implemented relatively efficiently used integer operations.\n",
    "For instance, we can use the first four bits of an ```int8``` value to represent the digits before the comma, and the last four bits to represent fractional digits that come after the comma:\n",
    "\n",
    "```\n",
    "Decimal: 0.5       + 1.25      = 1.75\n",
    "Binary:  0000.1000 + 0001.0100 = 0001.1100\n",
    "\n",
    "```\n",
    "\n",
    "A fixed-point addition can be implemented by simple integer addition and a fixed-point multiplication by an integer multiplication followed by a bit-wise shift operation.\n",
    "\n",
    "Obviously, the precision achieved with an 8-bit fixed-point format is not enough for training a neural network. However, most types of layers can be quantized for inferencing without suffering a significant loss in accuracy.\n",
    "The quantization step itself rounds the ```float32``` weight values to their nearest corresponding fixed-point value.\n",
    "\n",
    "The clear advantages of running a network using ```int8``` is that:\n",
    "\n",
    "1. It requires less memory, which improves cache and memory bandwidth efficiency.\n",
    "2. Can run using more efficient integer operations\n",
    "\n",
    "In particular, a [2017 Google paper](https://arxiv.org/pdf/1704.04760.pdf) writes:\n",
    "\n",
    "> Eight-bit integer multiplies can be 6X less energy and 6X less area than IEEE 754 16-bit floating-point multiplies and the\n",
    "> advantage for integer addition is 13X in energy and 38X in area [Dal16].\n",
    "> \n",
    "> ***- ''In-Datacenter Performance Analysis of a Tensor Processing Unit'' - Jouppi et al.***\n",
    "\n",
    "\n",
    "Despite this relatively simple concept, there are several misconceptions and myths regarding quantized neural networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #1: Quantization is only necessary for ultra-low-power embedded systems\n",
    "\n",
    "Far from it. Datacenter applications currently benefit the most from quantization. \n",
    "For instance, the first generation of [Google's Tensor Processing Units (TPUs)](https://arxiv.org/pdf/1704.04760.pdf) only supported quantized networks. Computation units for floating-point arithmetic were only added in the [second generation](https://www.tomshardware.com/news/tpu-v2-google-machine-learning,35370.html).\n",
    "\n",
    "Likewise, Nvidia's [V100](https://www.microway.com/knowledge-center-articles/in-depth-comparison-of-nvidia-tesla-volta-gpu-accelerators/) and latest [A100](https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products) can\n",
    "perform four times as many ```int8``` tensor operations compared to ```float32``` operations per second (or twice as much ```int8``` as ```float16``` tensor operations per second).\n",
    "This means that you can **quadruple the throughput of your datacenter application** with quantization in a best-case scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #2: Quantization makes networks smaller but not faster\n",
    "\n",
    "As already hinted in the myth above, modern AI accelerators such as GPU and TPU can run integer operations faster than floating-point operations.\n",
    "Let's look at the compute performance of Nvidia's latest [A100 GPU](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-57d3f3f031fc4a539098d324364e298e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-57d3f3f031fc4a539098d324364e298e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-57d3f3f031fc4a539098d324364e298e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Data type\", \"legend\": null, \"scale\": {\"scheme\": \"dark2\"}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Data type\"}, {\"type\": \"quantitative\", \"field\": \"TOPS\"}, {\"type\": \"nominal\", \"field\": \"Comment\"}, {\"type\": \"nominal\", \"field\": \"Significand precision\"}, {\"type\": \"nominal\", \"field\": \"Exponent\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"labels\": false, \"title\": \"Data format\"}, \"field\": \"Data type\", \"sort\": [\"float64\", \"float32\", \"TensorFloat32\", \"float16\", \"bfloat16\", \"int8\"]}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Tera op/s\"}, \"field\": \"TOPS\"}}, \"height\": 300, \"title\": \"Nvidia A100 compute performance\", \"width\": 600}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -10, \"fontSize\": 16}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Data type\", \"legend\": null, \"scale\": {\"scheme\": \"dark2\"}}, \"text\": {\"type\": \"nominal\", \"field\": \"Data type\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Data type\"}, {\"type\": \"quantitative\", \"field\": \"TOPS\"}, {\"type\": \"nominal\", \"field\": \"Comment\"}, {\"type\": \"nominal\", \"field\": \"Significand precision\"}, {\"type\": \"nominal\", \"field\": \"Exponent\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"labels\": false, \"title\": \"Data format\"}, \"field\": \"Data type\", \"sort\": [\"float64\", \"float32\", \"TensorFloat32\", \"float16\", \"bfloat16\", \"int8\"]}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Tera op/s\"}, \"field\": \"TOPS\"}}, \"height\": 300, \"title\": \"Nvidia A100 compute performance\", \"width\": 600}], \"data\": {\"name\": \"data-150720172dd484d891e26d6e7b27f0a1\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-150720172dd484d891e26d6e7b27f0a1\": [{\"Data type\": \"float64\", \"TOPS\": 9.7, \"Significand precision\": \"52-bits\", \"Exponent\": \"11-bits\", \"Comment\": \"Double precision IEE-754 floating-point\"}, {\"Data type\": \"float32\", \"TOPS\": 19.5, \"Significand precision\": \"23-bits\", \"Exponent\": \"8-bits\", \"Comment\": \"Single precision IEE-754 floating-point\"}, {\"Data type\": \"TensorFloat32\", \"TOPS\": 156.0, \"Significand precision\": \"10-bits\", \"Exponent\": \"8-bits\", \"Comment\": \"32-bit floating-point format with reduced significand precision\"}, {\"Data type\": \"float16\", \"TOPS\": 312.0, \"Significand precision\": \"10-bits\", \"Exponent\": \"5-bits\", \"Comment\": \"Half precision IEE-754 floating-point\"}, {\"Data type\": \"bfloat16\", \"TOPS\": 312.0, \"Significand precision\": \"7-bits\", \"Exponent\": \"8-bits\", \"Comment\": \"16-bit brain-float format with larger range but reduced significand precision\"}, {\"Data type\": \"int8\", \"TOPS\": 624.0, \"Significand precision\": \"7-bits\", \"Exponent\": \"0-bits\", \"Comment\": \"8-bit integer format for fixed-point arithmetic\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "points = alt.Chart(df).mark_bar().encode(\n",
    "    alt.X('Data type',sort=list(df['Data type']),axis=alt.Axis(labels=False,title='Data format')),\n",
    "    y=alt.Y('TOPS',axis=alt.Axis(title=\"Tera op/s\")),\n",
    "    color=alt.Color('Data type',legend=None,scale=alt.Scale(scheme='dark2')),\n",
    "    tooltip=['Data type','TOPS','Comment', 'Significand precision','Exponent']\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300,\n",
    "    title=\"Nvidia A100 compute performance\"\n",
    ")\n",
    "\n",
    "#points\n",
    "text = points.mark_text(\n",
    "    align='center',\n",
    "    fontSize=16,\n",
    "    baseline='middle',\n",
    "    dy=-10\n",
    ").encode(\n",
    "    text='Data type'\n",
    ")\n",
    "points + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, quantization does not only make the network smaller, but makes them also **runs faster**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #3: Any layer in a neural network can be quantized\n",
    "\n",
    "Some types of layers do not tolerate quantization very well. For example, in [a discussion of an ICLR paper by Max Welling's group](https://openreview.net/forum?id=HkxjYoCqKX&noteId=rygmk1EDT7) we see that quantizing the first or the last layer of a network results in a considerable drop in accuracy. \n",
    "This gap does not entirely close even if we train the network using [quantization-aware training](https://arxiv.org/pdf/1712.05877.pdf) techniques.\n",
    "\n",
    "One trick often used to avoid this drop in accuracy is not to quantize the first and the last layer. \n",
    "As these two layers only take up a small fraction of the computations inside a network, running the first and the last layer with ```float32``` does not hurt throughput much, but significantly benefits the accuracy of the network. \n",
    "\n",
    "However, one some end-devices, this approach is not an option. For instance, [Google's Edge TPU](https://cloud.google.com/edge-tpu) only supports ```int8```. Therefore, in such cases, every layer of the network must be quantized to 8-bit integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #4: It's easy to compare different quantization approaches\n",
    "\n",
    "Comparing two different quantization methods is not a trivial job. \n",
    "Connecting to the discussion above, let's imagine we have a network and quantize it with two different methods to obtain network A and network B.\n",
    "While network A achieves a 90% accuracy by quantized all layers, network B achieves a 92% accuracy but leaves the first layer running with floating-point precision.\n",
    "\n",
    "Which method is better?\n",
    "\n",
    "The answer to this question depends on the context; which target device will the network run on?\n",
    "If it's a device without a floating-point unit such as the Edge TPU or a microcontroller, then method A is clearly better. \n",
    "Contrarily, if we plan to run the network on a V100 or A100 GPU, then method B might be the better approach.\n",
    "\n",
    "Another technique that causes a lot of misconceptions found in [the discussion of the ICLR paper by Max Welling's group](https://openreview.net/forum?id=HkxjYoCqKX&noteId=rygmk1EDT7) are **non-uniform quantization schemes**:\n",
    "Fixed-point formats partition the representable value range using a uniform grid, e.g., there are the same amount of intermediate values between 1.5 and 2.5 as between 2.5 and 3.5. \n",
    "Looking at the typical weight distribution of neural networks, we notice that they follow a Gaussian-like bell curve distribution with smaller values occurring more frequently than large weight values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 18s 11ms/step - loss: 0.2303 - accuracy: 0.9289 - val_loss: 0.0575 - val_accuracy: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2f4c484da0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "\n",
    "weights = model.get_weights()\n",
    "weights = [weights[0].flatten(),weights[2].flatten(),weights[4].flatten()]\n",
    "weights = np.concatenate(weights,axis=0)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.distplot(weights,hist=True,norm_hist=True)\n",
    "plt.title(\"Weight distribution of a 3-layer CNN trained on MNIST\")\n",
    "plt.xlabel(\"Weight value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our only objective is to represent the weight values as accurately as possible using only 8-bits per parameter, then a logarithmic grid provides a better choice than the uniform partitioning of fixed-point formats.\n",
    "Although such non-uniform quantization approaches can remarkably compress the weights of the network, they destroy the critical advantage of quantization: The enabling of efficient integer arithmetic.\n",
    "\n",
    "In essence, comparing different quantization methods can be quite challenging as there are many tradeoffs to be considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #5: A 8-bit network uses only 8-bit variables\n",
    "\n",
    "In an 8-bit quantized neural network, all weights and activations are 8-bit integer variables.\n",
    "However, when we run the network, we need 32-bits accumulation registers internally to represent intermediate results.\n",
    "\n",
    "Let's take a look at the C-like implementation of a typical ReLU neuron:\n",
    "\n",
    "```C\n",
    "float relu_neuron(float* w, float* x, size_t n){\n",
    "    float accumulator = 0;\n",
    "    for(size_t i=0;i<n;i++){\n",
    "        accumulator += w[i]*x[i];\n",
    "    }\n",
    "    accumulator = max(0,accumulator);\n",
    "    return accumulator;\n",
    "}\n",
    "```\n",
    "\n",
    "Essentially all variables, the weights, the inputs, and the accumulator, are represented by the 32-bit floating-point type.\n",
    "For a neural network in a fixed-point format, we need a wider integer accumulation register to perform the multiplications and summation.  \n",
    "\n",
    "\n",
    "```C\n",
    "int8_t relu_neuron(int8_t* w, int8_t* x, size_t n, int shift){\n",
    "    int32_t accumulator = 0;\n",
    "    for(size_t i=0;i<n;i++){\n",
    "        // Multiplying two 8-bit integers requires a 16-bit register\n",
    "        int16_t product = w[i]*x[i];\n",
    "        // Sum over several 16-bit integers requires an even larger register (e.g. int32_t)\n",
    "        accumulator += product;\n",
    "    }\n",
    "    // Fixed-point multiplication requires a shift\n",
    "    accumulator = accumulator>>shift;\n",
    "    \n",
    "    accumulator = max(0,accumulator);\n",
    "    // Project neuron value back to valid int8_t range\n",
    "    accumulator = min(INT8_MAX,accumulator);\n",
    "    // Cast 32-bit accumulator back to 8-bit integer\n",
    "    return (int8_t)accumulator;\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the parameter ```shift``` depends on the exact fixed-point format we employ, i.e., how many bits we use for storing the digits before and how many bits we use for storing the digits after the comma.\n",
    "\n",
    "Page 27 of [Nvidia's A100 whitepaper](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf) specifies precisely which numerical format uses which numerical type for representing the **input operands and the accumulation registers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although the concept of neural network quantization is relatively simple, there are subtle details that can cause misconceptions. \n",
    "In this post, we have busted five common myths regarding quantized neural networks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
